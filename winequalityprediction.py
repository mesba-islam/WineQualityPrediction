# -*- coding: utf-8 -*-
"""WineQualityPrediction.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1wIuuxox9-lA3wb9bFlP4PWsPnoH5LhBi

# importing libraries
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.ensemble import RandomForestClassifier
from sklearn.preprocessing import MinMaxScaler, StandardScaler
from sklearn.linear_model import LinearRegression, LogisticRegression
from sklearn.metrics import confusion_matrix, accuracy_score, classification_report, roc_curve, f1_score, precision_score, recall_score
from sklearn.tree import DecisionTreeClassifier
from sklearn.svm import SVC, SVR

import warnings

# settings for waring
warnings.filterwarnings('ignore')
"""

# importing libraries
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.ensemble import RandomForestClassifier
from sklearn.preprocessing import MinMaxScaler, StandardScaler
from sklearn.linear_model import LinearRegression, LogisticRegression
from sklearn.metrics import confusion_matrix, accuracy_score, classification_report, roc_curve, f1_score, precision_score, recall_score
from sklearn.tree import DecisionTreeClassifier
from sklearn.svm import SVC, SVR

import warnings

# settings for waring
warnings.filterwarnings('ignore')

"""Loading Dataset"""

dataset = pd.read_csv('/content/wine_quality_dataset.csv')


# making dataframe
df = pd.DataFrame(dataset)

# printing dataframe
df

"""Dataset Visualization"""

# showing graph for all columns data visualization
df_white.hist(bins = 30, figsize = (12, 12));

# setup config
plt.figure(figsize=(12, 10))

# creating heatmaps
sns.heatmap(df_white.corr(), cbar=True, square=True, fmt='.2f', annot=True, cmap='rainbow')

# displaying heatmaps
plt.show()

"""Cleaning & Cheacking"""

# displaying basic info
df.info()

# getting only white wines
df = df[df['type'] == 'white']

# drop column type
df = df.drop('type', axis = 1)

# copying without type column
df_white = df.copy()

# printing dataframe with only white wine
df_white

"""Exploratory Data Analysis(EDA)"""

# check null column
df_white.isnull().sum()

# to drop any rows that contain any null values
df_white.dropna(inplace=True)

# to drop the rows which all of it's values is any
# df_white.dropna(how='all', inplace=True)

# if you want to drop the columns not the rows you just set the axis to 1 like this:
# df_white.dropna(axis=1, inplace=True)

# checking if there are still any null values
df_white.isnull().sum()

# checking NaN and False value in dataset
df_white.isna().sum()

# printing dataframe details
df_white.describe()

# chart for each value in wine quality
sns.catplot(x='quality', data=df_white, kind = 'count')

"""Constructing Pair Plots"""

# copy dataframe for pair plots
df_white_for_plots = df_white.copy()

# set 'quality' column to 0 for values less than or equal to 5, and set to 1 for values greater than 5
df_white_for_plots['quality'] = (df_white_for_plots['quality'] > 5).astype(int)

# priting first ten rows
df_white_for_plots.head(10)

# create a new column 'wine_type' based on the 'quality' column
df_white_for_plots['type'] = df_white_for_plots['quality'].apply(lambda x: 'Good' if x > 0 else 'Bad')

# specify the columns for which you want to create pair plots
columns_of_interest = [
    'fixed acidity',
    'volatile acidity',
    'citric acid',
    'residual sugar',
    'quality',
    'alcohol',
    'sulphates',
    'pH',
    'density',
    'total sulfur dioxide',
    'chlorides'
]

# set the style
sns.set(style="ticks", palette="muted")

# create pair plots with different colors for different wine qualities
pair_plot = sns.pairplot(df_white_for_plots[columns_of_interest + ['type']], hue='type', palette={'Good': 'green', 'Bad': 'red'})

# show the plot
plt.show()

"""**Logistic Regression**<br>
Predict the Probability of Good Wine Quality
"""

# copy dataframe for model
df_white_for_good_wine = df_white.copy()

# create a binary target variable 'good_wine' based on the 'quality' column
df_white_for_good_wine['good_wine'] = df_white_for_good_wine['quality'].apply(lambda x: 1 if x > 5 else 0)

# features variable
XG = df_white_for_good_wine.drop(['quality', 'good_wine'], axis=1)

# target variable
yg = df_white_for_good_wine['good_wine']

# split the data into training and testing sets
XG_train, XG_test, yg_train, yg_test = train_test_split(XG, yg, test_size=0.2, random_state=42)

# create and fit a logistic regression model
model_lr_g = LogisticRegression()
model_lr_g.fit(XG_train, yg_train)

# make predictions on the test set
y_pred_lr_g = model_lr_g.predict(XG_test)

# evaluate the model
print("Classification Report:")
print(classification_report(yg_test, y_pred_lr_g))

"""**Confusion Metrix**"""

# visualize the confusion matrix
conf_mat = confusion_matrix(yg_test, y_pred_lr_g)
sns.heatmap(conf_mat, annot=True, fmt='d', cmap='Blues')
plt.xlabel('Predicted Label')
plt.ylabel('True Label')
plt.title('Confusion Matrix')
plt.show()

# interpret the model coefficients
coef_df = pd.DataFrame({'Feature': XG.columns, 'Coefficient': model_lr_g.coef_[0]})
coef_df = coef_df.sort_values(by='Coefficient', ascending=False)
print("Model Coefficients:")
print(coef_df)

"""Train & test"""

# copy dataframe for model
df_white_for_model = df_white.copy()

# making quality value as 1 and 0
df_white_for_model['quality'] = (df_white_for_model['quality'] > 5).astype(int)

# features data
X = df_white_for_model.drop(columns="quality")

# test data
y = df_white_for_model["quality"]

# split the data train and test
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)

# data normalization with standard scaler
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

# data normalization with min max scaler
# normalization = MinMaxScaler()
# x_train = normalization.fit_transform(X_train)
# x_test = normalization.fit_transform(X_test)

# printing divided data info
print("X Train : ", X_train.shape)
print("X Test  : ", X_test.shape)
print("Y Train : ", y_train.shape)
print("Y Test  : ", y_test.shape)

# initialize variable to compare models
models_comparison = {}

"""Train, Build, Test with Logistic Regression"""

# creating model for logistic regression
model_lr = LogisticRegression(random_state=0)

# fitting data to model
model_lr.fit(X_train, y_train)

# predict with test data
y_pred_lr = model_lr.predict(X_test)

# find accuracies
accuracies_lr = cross_val_score(estimator=model_lr, X=X_train, y=y_train, cv=5)

# pushing model result to compare
models_comparison[f"{str(model_lr)}"] = [
    accuracy_score(y_pred_lr, y_test),
    f1_score(y_pred_lr, y_test, average="macro"),
    precision_score(y_pred_lr, y_test, average="macro"),
    recall_score(y_pred_lr, y_test, average="macro"),
    (accuracies_lr.mean()),
]

# finding accuracy_score
accuracy_score_lr = accuracy_score(y_pred_lr, y_test)

# generating confusion matrix
confusion_matrix_lr = confusion_matrix(y_test, y_pred_lr)

# true positive
TP_lr = confusion_matrix_lr[1,1]

# true negatives
TN_lr = confusion_matrix_lr[0,0]

# false positives
FP_lr = confusion_matrix_lr[0,1]

# false negatives
FN_lr = confusion_matrix_lr[1,0]

# calculating accuracy
accuracy_lr = float(TP_lr + TN_lr) / float(TP_lr + TN_lr + FP_lr + FN_lr)

# calculating sensitivity
sensitivity_lr = TP_lr / float(TP_lr + FN_lr)

# calculating specificity
specificity_lr = TN_lr / float(TN_lr + FP_lr)

# printing confusion matrix
print(confusion_matrix_lr)

# confusion matrix visualization
sns.heatmap(confusion_matrix_lr, annot=True, fmt='d', cmap='Blues')
plt.xlabel('Predicted Label')
plt.ylabel('True Label')
plt.title('Confusion Matrix')
plt.show()

# printing classification report
print(classification_report(y_pred_lr, y_test))

# displaying results
print("True Positive (Logistic Regression): ", TP_lr)
print("False Positive (Logistic Regression): ", FP_lr)
print("True Nagetive (Logistic Regression): ", TN_lr)
print("False Nagetive (Logistic Regression): ", FN_lr)
print("Accuracy (Logistic Regression): ", accuracy_score_lr)
print("Accuracy (Logistic Regression): ", accuracy_lr)
print("Sensitivity (Logistic Regression): ", sensitivity_lr)
print("Specificity (Logistic Regression): ", specificity_lr)

# predict probabilities
pred_prob_lr = model_lr.predict_proba(X_test)

# roc curve for models
fpr_lr, tpr_lr, thresh_lr = roc_curve(y_test, pred_prob_lr[:,1], pos_label=1)

# roc curve for tpr = fpr
random_probs = [0 for i in range(len(y_test))]
p_fpr_lr, p_tpr_lr, _ = roc_curve(y_test, random_probs, pos_label=1)

# plot roc curves
plt.plot(fpr_lr, tpr_lr, linestyle='--', color='orange', label='Logistic Regression')
plt.plot(p_fpr_lr, p_tpr_lr, linestyle='--', color='blue')

# roc curve labeling
plt.title('ROC curve')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive rate')

# roc curve settings
plt.legend(loc='best')
plt.savefig('ROC', dpi=300)
plt.show();

"""Train, Build, Test with Decision Tree"""

# creating model for decision tree
model_dt = DecisionTreeClassifier(max_depth=50)

# fitting data to model
model_dt.fit(X_train, y_train)

# predict with test data
y_pred_dt = model_dt.predict(X_test)

# find accuracies
accuracies_dt = cross_val_score(estimator=model_dt, X=X_train, y=y_train, cv=5)

# pushing model result to compare
models_comparison[f"{str(model_dt)}"] = [
    accuracy_score(y_pred_dt, y_test),
    f1_score(y_pred_dt, y_test, average="macro"),
    precision_score(y_pred_dt, y_test, average="macro"),
    recall_score(y_pred_dt, y_test, average="macro"),
    (accuracies_dt.mean()),
]

# finding accuracy_score
accuracy_score_dt = accuracy_score(y_pred_dt, y_test)

# generating confusion matrix
confusion_matrix_dt = confusion_matrix(y_test, y_pred_dt)

# true positive
TP_dt = confusion_matrix_dt[1,1]

# true negatives
TN_dt = confusion_matrix_dt[0,0]

# false positives
FP_dt = confusion_matrix_dt[0,1]

# false negatives
FN_dt = confusion_matrix_dt[1,0]

# calculating accuracy
accuracy_dt = float(TP_dt + TN_dt) / float(TP_dt + TN_dt + FP_dt + FN_dt)

# calculating sensitivity
sensitivity_dt = TP_dt / float(TP_dt + FN_dt)

# calculating specificity
specificity_dt = TN_dt / float(TN_dt + FP_dt)

# printing confusion matrix
print(confusion_matrix_dt)

# confusion matrix visualization
sns.heatmap(confusion_matrix_dt, annot=True, fmt='d', cmap='Blues')
plt.xlabel('Predicted Label')
plt.ylabel('True Label')
plt.title('Confusion Matrix')
plt.show()

# printing classification report
print(classification_report(y_pred_dt, y_test))

# displaying results
print("True Positive (Decision Tree): ", TP_dt)
print("False Positive (Decision Tree): ", FP_dt)
print("True Nagetive (Decision Tree): ", TN_dt)
print("False Nagetive (Decision Tree): ", FN_dt)
print("Accuracy (Decision Tree): ", accuracy_score_dt)
print("Accuracy (Decision Tree): ", accuracy_dt)
print("Sensitivity (Decision Tree): ", sensitivity_dt)
print("Specificity (Decision Tree): ", specificity_dt)

# predict probabilities
pred_prob_dt = model_dt.predict_proba(X_test)

# roc curve for models
fpr_dt, tpr_dt, thresh_dt = roc_curve(y_test, pred_prob_dt[:,1], pos_label=1)

# roc curve for tpr = fpr
random_probs = [0 for i in range(len(y_test))]
p_fpr_dt, p_tpr_dt, _ = roc_curve(y_test, random_probs, pos_label=1)

# plot roc curves
plt.plot(fpr_dt, tpr_dt, linestyle='--', color='orange', label='Decision Tree')
plt.plot(p_fpr_dt, p_tpr_dt, linestyle='--', color='blue')

# roc curve labeling
plt.title('ROC curve')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive rate')

# roc curve settings
plt.legend(loc='best')
plt.savefig('ROC', dpi=300)
plt.show();

"""Train, Build, Test with Random Forest"""

# creating model for random forest
model_rf = RandomForestClassifier(random_state=0)

# fitting data to model
model_rf.fit(X_train, y_train)

# predict with test data
y_pred_rf = model_rf.predict(X_test)

# find accuracies
accuracies_rf = cross_val_score(estimator=model_rf, X=X_train, y=y_train, cv=5)

# pushing model result to compare
models_comparison[f"{str(model_rf)}"] = [
    accuracy_score(y_pred_rf, y_test),
    f1_score(y_pred_rf, y_test, average="macro"),
    precision_score(y_pred_rf, y_test, average="macro"),
    recall_score(y_pred_rf, y_test, average="macro"),
    (accuracies_rf.mean()),
]

# finding accuracy_score
accuracy_score_rf = accuracy_score(y_pred_rf, y_test)

# generating confusion matrix
confusion_matrix_rf = confusion_matrix(y_test, y_pred_rf)

# true positive
TP_rf = confusion_matrix_rf[1,1]

# true negatives
TN_rf = confusion_matrix_rf[0,0]

# false positives
FP_rf = confusion_matrix_rf[0,1]

# false negatives
FN_rf = confusion_matrix_rf[1,0]

# calculating accuracy
accuracy_rf = float(TP_rf + TN_rf) / float(TP_rf + TN_rf + FP_rf + FN_rf)

# calculating sensitivity
sensitivity_rf = TP_rf / float(TP_rf + FN_rf)

# calculating specificity
specificity_rf = TN_rf / float(TN_rf + FP_rf)

# printing confusion matrix
print(confusion_matrix_rf)

# confusion matrix visualization
sns.heatmap(confusion_matrix_rf, annot=True, fmt='d', cmap='Blues')
plt.xlabel('Predicted Label')
plt.ylabel('True Label')
plt.title('Confusion Matrix')
plt.show()

# printing classification report
print(classification_report(y_pred_rf, y_test))

# displaying results
print("True Positive (Random Forest): ", TP_rf)
print("False Positive (Random Forest): ", FP_rf)
print("True Nagetive (Random Forest): ", TN_rf)
print("False Nagetive (Random Forest): ", FN_rf)
print("Accuracy (Random Forest): ", accuracy_score_rf)
print("Accuracy (Random Forest): ", accuracy_rf)
print("Sensitivity (Random Forest): ", sensitivity_rf)
print("Specificity (Random Forest): ", specificity_rf)

# predict probabilities
pred_prob_rf = model_rf.predict_proba(X_test)

# roc curve for models
fpr_rf, tpr_rf, thresh_rf = roc_curve(y_test, pred_prob_rf[:,1], pos_label=1)

# roc curve for tpr = fpr
random_probs = [0 for i in range(len(y_test))]
p_fpr_rf, p_tpr_rf, _ = roc_curve(y_test, random_probs, pos_label=1)

# plot roc curves
plt.plot(fpr_rf, tpr_rf, linestyle='--', color='orange', label='Random Forest')
plt.plot(p_fpr_rf, p_tpr_rf, linestyle='--', color='blue')

# roc curve labeling
plt.title('ROC curve')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive rate')

# roc curve settings
plt.legend(loc='best')
plt.savefig('ROC', dpi=300)
plt.show();

"""Train, Build, Test with Support Vector Machine"""

# creating model for support vector machine
model_sv = SVC(random_state=0, probability=True)
# model_sv = SVC(C=50, kernel="rbf", probability=True)
# model_sv = SVR(degree=1, coef0=1, tol=0.001, C=1.5, epsilon=0.001)

# fitting data to model
model_sv.fit(X_train, y_train)

# predict with test data
y_pred_sv = model_sv.predict(X_test)

# find accuracies
accuracies_sv = cross_val_score(estimator=model_sv, X=X_train, y=y_train, cv=5)

# pushing model result to compare
models_comparison[f"{str(model_sv)}"] = [
    accuracy_score(y_pred_sv, y_test),
    f1_score(y_pred_sv, y_test, average="macro"),
    precision_score(y_pred_sv, y_test, average="macro"),
    recall_score(y_pred_sv, y_test, average="macro"),
    (accuracies_sv.mean()),
]

# finding accuracy_score
accuracy_score_sv = accuracy_score(y_pred_sv, y_test)

# generating confusion matrix
confusion_matrix_sv = confusion_matrix(y_test, y_pred_sv)

# true positive
TP_sv = confusion_matrix_sv[1,1]

# true negatives
TN_sv = confusion_matrix_sv[0,0]

# false positives
FP_sv = confusion_matrix_sv[0,1]

# false negatives
FN_sv = confusion_matrix_sv[1,0]

# calculating accuracy
accuracy_sv = float(TP_sv + TN_sv) / float(TP_sv + TN_sv + FP_sv + FN_sv)

# calculating sensitivity
sensitivity_sv = TP_sv / float(TP_sv + FN_sv)

# calculating specificity
specificity_sv = TN_sv / float(TN_sv + FP_sv)

# printing confusion matrix
print(confusion_matrix_sv)

# confusion matrix visualization
sns.heatmap(confusion_matrix_sv, annot=True, fmt='d', cmap='Blues')
plt.xlabel('Predicted Label')
plt.ylabel('True Label')
plt.title('Confusion Matrix')
plt.show()

# printing classification report
print(classification_report(y_pred_sv, y_test))

# displaying results
print("True Positive (Support Vector): ", TP_sv)
print("False Positive (Support Vector): ", FP_sv)
print("True Nagetive (Support Vector): ", TN_sv)
print("False Nagetive (Support Vector): ", FN_sv)
print("Accuracy (Support Vector): ", accuracy_score_sv)
print("Accuracy (Support Vector): ", accuracy_sv)
print("Sensitivity (Support Vector): ", sensitivity_sv)
print("Specificity (Support Vector): ", specificity_sv)

# predict probabilities
pred_prob_sv = model_sv.predict_proba(X_test)

# roc curve for models
fpr_sv, tpr_sv, thresh_sv = roc_curve(y_test, pred_prob_sv[:,1], pos_label=1)

# roc curve for tpr = fpr
random_probs = [0 for i in range(len(y_test))]
p_fpr_sv, p_tpr_sv, _ = roc_curve(y_test, random_probs, pos_label=1)

# plot roc curves
plt.plot(fpr_sv, tpr_sv, linestyle='--', color='orange', label='Support Vector')
plt.plot(p_fpr_sv, p_tpr_sv, linestyle='--', color='blue')

# roc curve labeling
plt.title('ROC curve')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive rate')

# roc curve settings
plt.legend(loc='best')
plt.savefig('ROC', dpi=300)
plt.show();

"""**Find Best Performer Model**"""

models_df = pd.DataFrame(models_comparison).T

models_df.columns = [
    "Model Accuracy",
    "Model F1-Score",
    "Precision",
    "Recall",
    "CV Accuracy"
]

models_df = models_df.sort_values(by="Model F1-Score", ascending=False)
models_df.style.format("{:.2%}").background_gradient(cmap="Blues")